import json
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.neural_network import MLPClassifier 
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import joblib 
import os 
import io 

# --- Configuration ---
LOG_FILE_PATHS = [
    os.path.join('training_data', 'coraza-audit-benign.csv'),
    os.path.join('training_data', 'coraza-audit-sqli.csv'),
    os.path.join('training_data', 'coraza-audit-xss.csv')
]
MODEL_OUTPUT_PATH = os.path.join('..', 'ai-microservice', 'mlp_malicious_traffic_model.joblib')
PREPROCESSOR_OUTPUT_PATH = os.path.join('..', 'ai-microservice', 'mlp_malicious_traffic_preprocessor.joblib')

COLUMNS_TO_DROP = ['Timestamp', 'TransactionID', 'ClientIP', 'ClientPort', 'ServerIP', 'ServerPort',
                   'ResponseProtocol', 'ResponseHeaders', 'ResponseBody', 'WAFInterrupted', 'InterruptionRuleID',
                   'InterruptionStatus', 'InterruptionAction', 'MatchedRulesCount', 'MatchedRulesIDs', 'MatchedRulesMessages',
                   'MatchedRulesTags', 'AIScore', 'AIVerdict']

CLEANED_DATA_OUTPUT_PATH = os.path.join('training_data', 'coraza-audit-cleaned.csv')

problematic_endings = [
    "Failed to write CSV record: short write",
    "CSV writer error: short write"
]

# --- 1. Data Loading ---
def load_and_clean_data(log_file_paths, columns_to_drop=None):
    """
    Loads and merges CSV log entries from the specified list of files,
    then performs initial data cleaning.
    """
    all_dfs = []
    for log_file in log_file_paths:
        try:
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                raw_lines = f.readlines()

            if not raw_lines:
                print(f"Warning: {log_file} is empty. Skipping.")
                continue

            header_line = raw_lines[0]
            data_lines = raw_lines[1:]
            filtered_data_lines = []

            for data_line in data_lines:
                stripped_line = data_line.rstrip('\r\n') 
                keep_this_line = True
                
                for ending in problematic_endings:
                    if stripped_line.endswith(ending):
                        keep_this_line = False
                        break
                
                if keep_this_line:
                    filtered_data_lines.append(data_line)

            processed_content = [header_line] + filtered_data_lines
            if len(processed_content) <= 1:
                print(f"Warning: No valid CSV-like data lines found in {log_file}. Skipping.")
                continue

            data_io = io.StringIO("".join(processed_content))
            df = pd.read_csv(data_io, on_bad_lines='skip')
            all_dfs.append(df)
            print(f"Successfully loaded and pre-filtered {log_file}")

        except FileNotFoundError:
            print(f"Warning: Log file not found at {log_file}. Skipping.")
        except Exception as e:
            print(f"Error processing CSV file {log_file}: {e}. Skipping.")

    if not all_dfs:
        print("No data loaded from any specified files.")
        return pd.DataFrame()

    df = pd.concat(all_dfs, ignore_index=True)
    print(f"Loaded {len(df)} raw log entries from all files.")

    # --- Data Cleaning ---
    print("\nPerforming data cleaning...")
    initial_rows = len(df)

    if not df.empty and len(df.columns) > 0:
        first_column_name = df.columns[0]
        df_filtered = df[~df[first_column_name].astype(str).str.contains("Failed|Error", case=False, na=False)]
        rows_removed_first_field = initial_rows - len(df_filtered)
        if rows_removed_first_field > 0:
            print(f"Removed {rows_removed_first_field} rows where the first field contained 'Failed' or 'Error'.")
            df = df_filtered

    df.drop_duplicates(inplace=True)
    print(f"Removed {initial_rows - len(df)} duplicate rows.")

    if 'AIVerdictLabel' in df.columns:
        df = df[df['AIVerdictLabel'].isin(['benign', 'malicious'])]
        print(f"Removed {initial_rows - len(df)} rows with invalid AIVerdictLabel.")
    
    critical_columns = ['RequestURI', 'RequestMethod']
    existing_critical_columns = [col for col in critical_columns if col in df.columns]
    if existing_critical_columns:
        df.dropna(subset=existing_critical_columns, inplace=True)
        print(f"Removed rows with missing critical features.")

    if 'RequestMethod' in df.columns:
        df['RequestMethod'] = df['RequestMethod'].astype(str).str.upper()

    if columns_to_drop:
        cols_to_drop_existing = [col for col in columns_to_drop if col in df.columns]
        if cols_to_drop_existing:
            df.drop(columns=cols_to_drop_existing, inplace=True)
            print(f"Removed columns: {', '.join(cols_to_drop_existing)}")

    print(f"Remaining {len(df)} entries after cleaning.")
    return df

# --- 2. Feature Extraction and Preprocessing ---
def preprocess_data(df):
    df['label'] = df['AIVerdictLabel'].apply(lambda x: 1 if x == 'malicious' else 0)
    df['request_method'] = df.get('RequestMethod', 'UNKNOWN').fillna('')
    df['request_uri_path'] = df.get('RequestURIPath', '').fillna('')
    df['request_uri_query'] = df.get('RequestURIQuery', '').fillna('')
    df['request_body'] = df.get('RequestBody', '').fillna('')
    df['user_agent'] = df.get('UserAgent', '').fillna('')
    df['request_length'] = df.get('RequestLength', 0)
    df['path_length'] = df.get('PathLength', 0)
    df['query_length'] = df.get('QueryLength', 0)

    text_features = ['request_uri_path', 'request_uri_query', 'request_body', 'user_agent']
    categorical_features = ['request_method']
    numerical_features = ['request_length', 'path_length', 'query_length']

    preprocessor = ColumnTransformer(
        transformers=[
            ('text_uri_path', TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(2, 4)), 'request_uri_path'),
            ('text_uri_query', TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(2, 4)), 'request_uri_query'),
            ('text_body', TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(2, 4)), 'request_body'),
            ('text_user_agent', TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(2, 4)), 'user_agent'),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
            ('num', 'passthrough', numerical_features)
        ],
        remainder='drop'
    )

    X_processed = preprocessor.fit_transform(df)
    return preprocessor, X_processed, df['label']

# --- 3. Model Training (MLPClassifier) ---
def train_model(X_train, y_train):
    model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=200, random_state=42, activation='relu', solver='adam')
    model.fit(X_train, y_train)
    return model

# --- 4. Model Evaluation ---
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precision: {precision_score(y_test, y_pred):.4f}")
    print(f"Recall: {recall_score(y_test, y_pred):.4f}")
    print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['benign', 'malicious']))

# --- 5. Model Export ---
def save_model_and_preprocessor(model, preprocessor, model_path, preprocessor_path):
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    os.makedirs(os.path.dirname(preprocessor_path), exist_ok=True)
    joblib.dump(model, model_path)
    joblib.dump(preprocessor, preprocessor_path)
if __name__ == "__main__":
    main()
from sklearn.model_selection import cross_val_score

# Cross-validation
scores = cross_val_score(knn, X, y, cv=5)  # 5-fold cross-validation
print("Cross-validation scores:", scores)
print("Mean cross-validation score:", scores.mean())
